mkdir: cannot create directory ‘resultsdump/job_taus40d200_18116628’: File exists
2024-02-02 01:21:49.680568: W external/tsl/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 28.31GiB (rounded to 30396335104)requested by op 
2024-02-02 01:21:49.680882: W external/tsl/tsl/framework/bfc_allocator.cc:497] **__________________________________________________________________________________________________
2024-02-02 01:21:49.681301: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2732] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 30396334856 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:  393.90MiB
              constant allocation:        12B
        maybe_live_out allocation:    3.78MiB
     preallocated temp allocation:   28.31GiB
  preallocated temp fragmentation:         0B (0.00%)
                 total allocation:   28.70GiB
              total fragmentation:    3.79MiB (0.01%)
Peak buffers:
	Buffer 1:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/jvp(Transformer)/TransformerBlock_0/LayerNorm_0/add" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=183 deduplicated_name="fusion.110"
		XLA Label: fusion
		Shape: f32[60800,41,200]
		==========================

	Buffer 2:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/jvp(Transformer)/TransformerBlock_1/LayerNorm_0/add" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=183 deduplicated_name="fusion.110"
		XLA Label: fusion
		Shape: f32[60800,41,200]
		==========================

	Buffer 3:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/jvp(Transformer)/TransformerBlock_0/LayerNorm_1/add" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=193 deduplicated_name="fusion.110"
		XLA Label: fusion
		Shape: f32[60800,41,200]
		==========================

	Buffer 4:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/TransformerBlock_0/LayerNorm_0/add_any" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=183 deduplicated_name="fusion.84"
		XLA Label: fusion
		Shape: f32[60800,41,200]
		==========================

	Buffer 5:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/TransformerBlock_0/Dense_0/reduce_sum[axes=(0, 1)]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=187
		XLA Label: fusion
		Shape: f32[60800,41,200]
		==========================

	Buffer 6:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/TransformerBlock_1/SingleHeadSelfAttention_0/...qd,...kd->...qk/dot_general[dimension_numbers=(((1,), (1,)), ((0,), (0,))) precision=None preferred_element_type=float32]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=109
		XLA Label: custom-call
		Shape: f32[60800,41,200]
		==========================

	Buffer 7:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/TransformerBlock_1/SingleHeadSelfAttention_0/transpose[permutation=(0, 2, 1)]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=122
		XLA Label: custom-call
		Shape: f32[60800,41,200]
		==========================

	Buffer 8:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/TransformerBlock_1/SingleHeadSelfAttention_0/...qd,...kd->...qk/dot_general[dimension_numbers=(((2,), (1,)), ((0,), (0,))) precision=None preferred_element_type=float32]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=109
		XLA Label: custom-call
		Shape: f32[60800,41,200]
		==========================

	Buffer 9:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/TransformerBlock_1/Dense_0/reduce_sum[axes=(0, 1)]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=187
		XLA Label: fusion
		Shape: f32[60800,41,200]
		==========================

	Buffer 10:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/jvp(Transformer)/TransformerBlock_0/SingleHeadSelfAttention_0/value/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=106
		XLA Label: custom-call
		Shape: f32[2492800,200]
		==========================

	Buffer 11:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/jvp(Transformer)/TransformerBlock_0/SingleHeadSelfAttention_0/key/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=105
		XLA Label: custom-call
		Shape: f32[2492800,200]
		==========================

	Buffer 12:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/jvp(Transformer)/TransformerBlock_0/SingleHeadSelfAttention_0/query/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=104
		XLA Label: custom-call
		Shape: f32[2492800,200]
		==========================

	Buffer 13:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/jvp(Transformer)/Dense_0/add" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=228
		XLA Label: fusion
		Shape: f32[2492800,200]
		==========================

	Buffer 14:
		Size: 389.88MiB
		Operator: op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/TransformerBlock_0/SingleHeadSelfAttention_0/dot_general[dimension_numbers=(((2,), (2,)), ((0,), (0,))) precision=None preferred_element_type=float32]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=122
		XLA Label: custom-call
		Shape: f32[60800,41,41]
		==========================

	Buffer 15:
		Size: 389.88MiB
		Operator: op_name="jit(train_step)/jit(main)/jvp(Transformer)/TransformerBlock_0/SingleHeadSelfAttention_0/reduce_sum[axes=(2,)]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=116 deduplicated_name="fusion.149"
		XLA Label: fusion
		Shape: f32[60800,41,41]
		==========================


Traceback (most recent call last):
  File "/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/tau.py", line 35, in <module>
    state, hist = train(config, data_iter=iter(linobject), loss='mse', test_every=1000, train_iters=500000, optim=optax.adamw,lr=1e-4)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../traintheory.py", line 177, in train
    state = train_step(state, mybatch, loss=loss, l1_weight=l1_weight, l2_weight=l2_weight)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 30396334856 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:  393.90MiB
              constant allocation:        12B
        maybe_live_out allocation:    3.78MiB
     preallocated temp allocation:   28.31GiB
  preallocated temp fragmentation:         0B (0.00%)
                 total allocation:   28.70GiB
              total fragmentation:    3.79MiB (0.01%)
Peak buffers:
	Buffer 1:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/jvp(Transformer)/TransformerBlock_0/LayerNorm_0/add" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=183 deduplicated_name="fusion.110"
		XLA Label: fusion
		Shape: f32[60800,41,200]
		==========================

	Buffer 2:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/jvp(Transformer)/TransformerBlock_1/LayerNorm_0/add" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=183 deduplicated_name="fusion.110"
		XLA Label: fusion
		Shape: f32[60800,41,200]
		==========================

	Buffer 3:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/jvp(Transformer)/TransformerBlock_0/LayerNorm_1/add" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=193 deduplicated_name="fusion.110"
		XLA Label: fusion
		Shape: f32[60800,41,200]
		==========================

	Buffer 4:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/TransformerBlock_0/LayerNorm_0/add_any" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=183 deduplicated_name="fusion.84"
		XLA Label: fusion
		Shape: f32[60800,41,200]
		==========================

	Buffer 5:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/TransformerBlock_0/Dense_0/reduce_sum[axes=(0, 1)]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=187
		XLA Label: fusion
		Shape: f32[60800,41,200]
		==========================

	Buffer 6:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/TransformerBlock_1/SingleHeadSelfAttention_0/...qd,...kd->...qk/dot_general[dimension_numbers=(((1,), (1,)), ((0,), (0,))) precision=None preferred_element_type=float32]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=109
		XLA Label: custom-call
		Shape: f32[60800,41,200]
		==========================

	Buffer 7:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/TransformerBlock_1/SingleHeadSelfAttention_0/transpose[permutation=(0, 2, 1)]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=122
		XLA Label: custom-call
		Shape: f32[60800,41,200]
		==========================

	Buffer 8:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/TransformerBlock_1/SingleHeadSelfAttention_0/...qd,...kd->...qk/dot_general[dimension_numbers=(((2,), (1,)), ((0,), (0,))) precision=None preferred_element_type=float32]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=109
		XLA Label: custom-call
		Shape: f32[60800,41,200]
		==========================

	Buffer 9:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/TransformerBlock_1/Dense_0/reduce_sum[axes=(0, 1)]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=187
		XLA Label: fusion
		Shape: f32[60800,41,200]
		==========================

	Buffer 10:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/jvp(Transformer)/TransformerBlock_0/SingleHeadSelfAttention_0/value/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=106
		XLA Label: custom-call
		Shape: f32[2492800,200]
		==========================

	Buffer 11:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/jvp(Transformer)/TransformerBlock_0/SingleHeadSelfAttention_0/key/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=105
		XLA Label: custom-call
		Shape: f32[2492800,200]
		==========================

	Buffer 12:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/jvp(Transformer)/TransformerBlock_0/SingleHeadSelfAttention_0/query/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=104
		XLA Label: custom-call
		Shape: f32[2492800,200]
		==========================

	Buffer 13:
		Size: 1.86GiB
		Operator: op_name="jit(train_step)/jit(main)/jvp(Transformer)/Dense_0/add" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=228
		XLA Label: fusion
		Shape: f32[2492800,200]
		==========================

	Buffer 14:
		Size: 389.88MiB
		Operator: op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/TransformerBlock_0/SingleHeadSelfAttention_0/dot_general[dimension_numbers=(((2,), (2,)), ((0,), (0,))) precision=None preferred_element_type=float32]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=122
		XLA Label: custom-call
		Shape: f32[60800,41,41]
		==========================

	Buffer 15:
		Size: 389.88MiB
		Operator: op_name="jit(train_step)/jit(main)/jvp(Transformer)/TransformerBlock_0/SingleHeadSelfAttention_0/reduce_sum[axes=(2,)]" source_file="/n/holyscratch01/pehlevan_lab/Lab/mletey/ICLexperiments/experiment/remote/sizesweep/../../../model/transformer.py" source_line=116 deduplicated_name="fusion.149"
		XLA Label: fusion
		Shape: f32[60800,41,41]
		==========================


--------------------
For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.
